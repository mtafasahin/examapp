name: Build & Deploy (Azure VM + ACR)

on:
  workflow_dispatch:
    inputs:
      mode:
        description: 'Run mode'
        type: choice
        required: true
        default: build_and_deploy
        options:
          - build_and_deploy
          - build_only
          - deploy_only
          - bootstrap_minimal
          - diagnose_tls
      target:
        description: 'Deploy target'
        type: choice
        required: true
        default: staging
        options:
          - staging
          - prod
      service:
        description: 'Service to build & push (use all for full rebuild)'
        type: choice
        required: true
        default: all
        options:
          - all
          - exam-dotnet-api
          - auth-api
          - exam-badge-api
          - ocelot-gateway
          - exam-outbox-publisher
          - angular-app
          - auth-ui
          - question-detector
      image_tag:
        description: 'Optional image tag override (default: git sha)'
        type: string
        required: false

permissions:
  contents: read
  id-token: write

concurrency:
  group: examapp-${{ github.event_name == 'workflow_dispatch' && inputs.target || startsWith(github.ref, 'refs/tags/') && 'prod' || 'staging' }}
  cancel-in-progress: true

env:
  TARGET: ${{ github.event_name == 'workflow_dispatch' && inputs.target || startsWith(github.ref, 'refs/tags/') && 'prod' || 'staging' }}
  SERVICE: ${{ github.event_name == 'workflow_dispatch' && inputs.service || 'all' }}
  IMAGE_TAG: ${{ github.event_name == 'workflow_dispatch' && inputs.image_tag || github.sha }}
  IMAGE_TAG_INPUT: ${{ github.event_name == 'workflow_dispatch' && inputs.image_tag || '' }}

jobs:
  build_and_push:
    runs-on: ubuntu-latest
    if: ${{ github.event_name != 'workflow_dispatch' || (inputs.mode != 'deploy_only' && inputs.mode != 'bootstrap_minimal' && inputs.mode != 'diagnose_tls') }}
    environment: ${{ (github.event_name == 'workflow_dispatch' && inputs.target == 'prod' && 'production') || (startsWith(github.ref, 'refs/tags/') && 'production') || 'staging' }}
    outputs:
      acr_login_server: ${{ steps.acr.outputs.login_server }}
      services_to_deploy: ${{ steps.changes.outputs.services }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Detect changed services
        id: changes
        shell: bash
        run: |
          set -euo pipefail

          # Manual-only workflow: always use the selected input service.
          svc="${SERVICE:-all}"
          if [[ -z "$svc" ]]; then
            svc="all"
          fi
          echo "services=${svc}" >> "$GITHUB_OUTPUT"

      - name: Free disk space on runner
        if: ${{ steps.changes.outputs.services != '__none__' }}
        shell: bash
        run: |
          set -euo pipefail
          echo "Disk before cleanup:" && df -h

          # GitHub-hosted runners come with large preinstalled toolchains.
          # This workflow builds inside Docker, so we can safely remove these to avoid
          # intermittent 'no space left on device' during image pull/extract/build.
          sudo rm -rf /usr/share/dotnet /opt/ghc /usr/local/lib/android || true
          sudo apt-get clean || true

          # Ensure Docker has maximum headroom for layer extraction/buildkit.
          docker system prune -af || true
          docker builder prune -af || true

          echo "Disk after cleanup:" && df -h

      - name: Validate Azure secrets present
        if: ${{ steps.changes.outputs.services != '__none__' }}
        shell: bash
        run: |
          set -euo pipefail
          missing=0
          if [[ -z "${{ secrets.AZURE_CLIENT_ID }}" ]]; then
            echo "::error::Missing secret AZURE_CLIENT_ID (repo or environment)"
            missing=1
          fi
          if [[ -z "${{ secrets.AZURE_TENANT_ID }}" ]]; then
            echo "::error::Missing secret AZURE_TENANT_ID (repo or environment)"
            missing=1
          fi
          if [[ -z "${{ secrets.AZURE_SUBSCRIPTION_ID }}" ]]; then
            echo "::error::Missing secret AZURE_SUBSCRIPTION_ID (repo or environment)"
            missing=1
          fi

          # For ACR we accept either:
          # - Azure CLI path: ACR_NAME (or ACR_LOGIN_SERVER to derive name)
          # - Docker login path: ACR_LOGIN_SERVER + ACR_USERNAME + ACR_PASSWORD
          if [[ -z "${{ secrets.ACR_NAME }}" && -z "${{ secrets.ACR_LOGIN_SERVER }}" ]]; then
            echo "::error::Missing secret ACR_NAME or ACR_LOGIN_SERVER (repo or environment)"
            missing=1
          fi

          if [[ -n "${{ secrets.ACR_LOGIN_SERVER }}" ]]; then
            acr_ls=$(printf '%s' "${{ secrets.ACR_LOGIN_SERVER }}" | tr -d ' \t\r\n' | sed -E 's#^https?://##' | sed -E 's#/*$##' | tr '[:upper:]' '[:lower:]')
            if ! echo "$acr_ls" | grep -Eq '^[a-z0-9][a-z0-9-]*\.azurecr\.io$'; then
              echo "::error::Secret ACR_LOGIN_SERVER must look like 'sorukutusuacr.azurecr.io'. Got: $acr_ls"
              missing=1
            fi
          fi

          if [[ -n "${{ secrets.ACR_USERNAME }}" || -n "${{ secrets.ACR_PASSWORD }}" ]]; then
            if [[ -z "${{ secrets.ACR_USERNAME }}" || -z "${{ secrets.ACR_PASSWORD }}" ]]; then
              echo "::error::Provide both ACR_USERNAME and ACR_PASSWORD (or neither)"
              missing=1
            fi
            if [[ -z "${{ secrets.ACR_LOGIN_SERVER }}" ]]; then
              echo "::error::ACR_LOGIN_SERVER is required when using ACR_USERNAME/ACR_PASSWORD"
              missing=1
            fi
          fi
          exit $missing

      # Option A (recommended): Azure OIDC login (no long-lived secrets)
      - name: Azure login (OIDC)
        if: ${{ steps.changes.outputs.services != '__none__' }}
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      - name: ACR login
        if: ${{ steps.changes.outputs.services != '__none__' }}
        shell: bash
        run: |
          set -euo pipefail
          ACR_NAME="${{ secrets.ACR_NAME }}"
          ACR_LOGIN_SERVER="${{ secrets.ACR_LOGIN_SERVER }}"
          ACR_USERNAME="${{ secrets.ACR_USERNAME }}"
          ACR_PASSWORD="${{ secrets.ACR_PASSWORD }}"

          # Prefer direct Docker login if admin creds are available; this avoids subscription-scoped ACR lookups.
          if [[ -n "$ACR_LOGIN_SERVER" && -n "$ACR_USERNAME" && -n "$ACR_PASSWORD" ]]; then
            echo "$ACR_PASSWORD" | docker login "$ACR_LOGIN_SERVER" -u "$ACR_USERNAME" --password-stdin
            exit 0
          fi

          # Fallback to Azure CLI login (requires the registry to exist in the active subscription).
          if [[ -z "$ACR_NAME" && -n "$ACR_LOGIN_SERVER" ]]; then
            ACR_NAME="${ACR_LOGIN_SERVER%%.azurecr.io}"
          fi

          if [[ -z "$ACR_NAME" ]]; then
            echo "::error::ACR registry name is empty. Set secret ACR_NAME (e.g. sorukutusuacr) or set ACR_LOGIN_SERVER + ACR_USERNAME + ACR_PASSWORD to use docker login."
            exit 1
          fi

          az acr login --name "$ACR_NAME"

      - name: Set ACR login server
        id: acr
        if: ${{ steps.changes.outputs.services != '__none__' }}
        shell: bash
        run: |
          set -euo pipefail
          ACR_NAME="${{ secrets.ACR_NAME }}"
          ACR_LOGIN_SERVER="${{ secrets.ACR_LOGIN_SERVER }}"

          if [[ -n "$ACR_LOGIN_SERVER" ]]; then
            LOGIN_SERVER="$ACR_LOGIN_SERVER"
          else
            if [[ -z "$ACR_NAME" ]]; then
              echo "::error::ACR_NAME is empty and ACR_LOGIN_SERVER not provided."
              exit 1
            fi
            LOGIN_SERVER=$(az acr show -n "$ACR_NAME" --query loginServer -o tsv)
          fi
          # Normalize to a docker-safe registry host (lowercase, no scheme, no trailing slash, no whitespace)
          LOGIN_SERVER=$(printf '%s' "$LOGIN_SERVER" | tr -d ' \t\r\n' | sed -E 's#^https?://##' | sed -E 's#/*$##' | tr '[:upper:]' '[:lower:]')
          if echo "$LOGIN_SERVER" | grep -Eq '[A-Z]'; then
            echo "::error::ACR login server must be lowercase. Got: $LOGIN_SERVER";
            exit 1;
          fi
          if echo "$LOGIN_SERVER" | grep -Eq '/'; then
            echo "::error::ACR login server must not contain a path (expected like 'sorukutusuacr.azurecr.io'). Got: $LOGIN_SERVER";
            exit 1;
          fi
          if echo "$LOGIN_SERVER" | grep -Eq '^https?://'; then
            echo "::error::ACR login server must not include scheme (https://). Got: $LOGIN_SERVER";
            exit 1;
          fi
          if ! echo "$LOGIN_SERVER" | grep -Eq '^[a-z0-9][a-z0-9-]*\.azurecr\.io$'; then
            echo "::error::ACR login server must look like '<name>.azurecr.io'. Got: $LOGIN_SERVER";
            exit 1;
          fi
          echo "login_server=$LOGIN_SERVER" >> $GITHUB_OUTPUT

      - name: Setup Docker Buildx
        if: ${{ steps.changes.outputs.services != '__none__' }}
        uses: docker/setup-buildx-action@v3

      - name: Build & push images
        if: ${{ steps.changes.outputs.services != '__none__' }}
        shell: bash
        run: |
          set -euo pipefail
          ACR="${{ steps.acr.outputs.login_server }}"
          ACR=$(printf '%s' "$ACR" | tr -d ' \t\r\n' | sed -E 's#^https?://##' | sed -E 's#/*$##' | tr '[:upper:]' '[:lower:]')
          if echo "$ACR" | grep -Eq '[A-Z]'; then
            echo "::error::ACR host must be lowercase. Got: $ACR";
            exit 1;
          fi
          if echo "$ACR" | grep -Eq '/'; then
            echo "::error::ACR host must not contain a path. Got: $ACR";
            exit 1;
          fi
          TAG="${IMAGE_TAG}"
          SERVICES="${{ steps.changes.outputs.services }}"

          should_build() {
            local name="$1"
            if [[ "$SERVICES" == "all" ]]; then
              return 0
            fi
            [[ " $SERVICES " == *" $name "* ]]
          }

          if should_build "exam-dotnet-api"; then
            docker buildx build --push \
              -f deploy/dockerfiles/dotnet-web.Dockerfile \
              --build-arg PROJECT_PATH=api/ExamApp.Api/ExamApp.Api.csproj \
              --build-arg APP_DLL=ExamApp.Api.dll \
              --build-arg EXPOSE_PORT=5079 \
              -t "$ACR/exam-dotnet-api:$TAG" \
              .
          fi

          if should_build "auth-api"; then
            docker buildx build --push \
              -f deploy/dockerfiles/dotnet-web.Dockerfile \
              --build-arg PROJECT_PATH=auth-api/ExamApp.Api.csproj \
              --build-arg APP_DLL=ExamApp.Api.dll \
              --build-arg EXPOSE_PORT=5079 \
              -t "$ACR/auth-api:$TAG" \
              .
          fi

          if should_build "exam-badge-api"; then
            docker buildx build --push \
              -f deploy/dockerfiles/dotnet-web.Dockerfile \
              --build-arg PROJECT_PATH=Services/BadgeService/BadgeService.csproj \
              --build-arg APP_DLL=BadgeService.dll \
              --build-arg EXPOSE_PORT=8006 \
              -t "$ACR/exam-badge-api:$TAG" \
              .
          fi

          if should_build "ocelot-gateway"; then
            docker buildx build --push \
              -f deploy/dockerfiles/dotnet-web.Dockerfile \
              --build-arg PROJECT_PATH=Services/Gateway/Gateway.csproj \
              --build-arg APP_DLL=Gateway.dll \
              --build-arg EXPOSE_PORT=5678 \
              -t "$ACR/ocelot-gateway:$TAG" \
              .
          fi

          if should_build "exam-outbox-publisher"; then
            docker buildx build --push \
              -f deploy/dockerfiles/dotnet-worker-net10-preview.Dockerfile \
              --build-arg PROJECT_PATH=Services/OutboxPublisher/OutboxPublisherService.csproj \
              --build-arg APP_DLL=OutboxPublisherService.dll \
              -t "$ACR/exam-outbox-publisher:$TAG" \
              .
          fi

          if should_build "angular-app"; then
            docker buildx build --push \
              -f deploy/dockerfiles/angular-spa.Dockerfile \
              --build-arg APP_DIR=ui \
              -t "$ACR/ui:$TAG" \
              .
          fi

          if should_build "auth-ui"; then
            docker buildx build --push \
              -f deploy/dockerfiles/angular-spa.Dockerfile \
              --build-arg APP_DIR=auth-ui \
              -t "$ACR/auth-ui:$TAG" \
              .
          fi

          if should_build "question-detector"; then
            docker buildx build --push \
              -f deploy/dockerfiles/question-detector.Dockerfile \
              -t "$ACR/exam-question-detector:$TAG" \
              .
          fi

  deploy_after_build:
    runs-on: ubuntu-latest
    needs: [build_and_push]
    if: ${{ (github.event_name != 'workflow_dispatch' || inputs.mode == 'build_and_deploy') && needs.build_and_push.outputs.services_to_deploy != '__none__' }}
    environment: ${{ (github.event_name == 'workflow_dispatch' && inputs.target == 'prod' && 'production') || (startsWith(github.ref, 'refs/tags/') && 'production') || 'staging' }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Validate VM deploy secrets present
        shell: bash
        run: |
          set -euo pipefail
          missing=0
          if [[ -z "${{ secrets.VM_HOST }}" ]]; then
            echo "::error::Missing secret VM_HOST (set it in repo secrets or in the selected environment secrets)"
            missing=1
          fi
          if [[ -z "${{ secrets.VM_USER }}" ]]; then
            echo "::error::Missing secret VM_USER"
            missing=1
          fi
          if [[ -z "${{ secrets.VM_SSH_KEY }}" ]]; then
            echo "::error::Missing secret VM_SSH_KEY"
            missing=1
          fi
          if [[ -z "${{ secrets.VM_DEPLOY_DIR }}" ]]; then
            echo "::error::Missing secret VM_DEPLOY_DIR"
            missing=1
          fi
          if [[ -z "${{ secrets.VM_ENV_FILE }}" ]]; then
            echo "::error::Missing secret VM_ENV_FILE"
            missing=1
          fi
          exit $missing

      - name: Ensure VM deploy dirs exist
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.VM_HOST }}
          username: ${{ secrets.VM_USER }}
          key: ${{ secrets.VM_SSH_KEY }}
          script: |
            set -e
            mkdir -p "${{ secrets.VM_DEPLOY_DIR }}" "${{ secrets.VM_DEPLOY_DIR }}/scripts" "${{ secrets.VM_DEPLOY_DIR }}/postgres/init"

            # Ensure deploy tree is writable by the SSH user (scp-action untars into target).
            if command -v sudo >/dev/null 2>&1; then
              sudo chown -R "${{ secrets.VM_USER }}":"${{ secrets.VM_USER }}" "${{ secrets.VM_DEPLOY_DIR }}" || true
              sudo chmod -R u+rwX,go+rX "${{ secrets.VM_DEPLOY_DIR }}" || true
            else
              chmod -R u+rwX,go+rX "${{ secrets.VM_DEPLOY_DIR }}" || true
            fi

      - name: Upload deploy assets to VM
        uses: appleboy/scp-action@v0.1.7
        with:
          host: ${{ secrets.VM_HOST }}
          username: ${{ secrets.VM_USER }}
          key: ${{ secrets.VM_SSH_KEY }}
          source: 'deploy/Caddyfile,deploy/docker-compose.prod.yml,deploy/docker-compose.prod.images.yml,deploy/docker-compose.pgadmin.yml'
          target: '${{ secrets.VM_DEPLOY_DIR }}'

      - name: Upload VM update script
        uses: appleboy/scp-action@v0.1.7
        with:
          host: ${{ secrets.VM_HOST }}
          username: ${{ secrets.VM_USER }}
          key: ${{ secrets.VM_SSH_KEY }}
          source: 'deploy/scripts/vm-update.sh'
          target: '${{ secrets.VM_DEPLOY_DIR }}/scripts'

      - name: Deploy on VM (SSH)
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.VM_HOST }}
          username: ${{ secrets.VM_USER }}
          key: ${{ secrets.VM_SSH_KEY }}
          script: |
            set -e
            cd "${{ secrets.VM_DEPLOY_DIR }}"

            echo "VM deploy dir: $(pwd)"
            echo "Caddyfile keycloak matcher preview:"
            if [ -f ./Caddyfile ]; then
              grep -n -E "@keycloak|handle @keycloak|X-Forwarded-Proto|reverse_proxy keycloak" ./Caddyfile || true
            fi

            ACR_LOGIN_SERVER_EFFECTIVE="${{ secrets.ACR_LOGIN_SERVER }}"
            if [ -z "$ACR_LOGIN_SERVER_EFFECTIVE" ]; then
              ACR_LOGIN_SERVER_EFFECTIVE="${{ needs.build_and_push.outputs.acr_login_server }}"
            fi
            if [ -z "$ACR_LOGIN_SERVER_EFFECTIVE" ]; then
              echo "ACR_LOGIN_SERVER is empty (set secret ACR_LOGIN_SERVER or ensure build job outputs it)." >&2
              exit 1
            fi

            ACR_LOGIN_SERVER_NORM=$(printf '%s' "$ACR_LOGIN_SERVER_EFFECTIVE" | tr -d ' \t\r\n' | sed -E 's#^https?://##' | sed -E 's#/*$##' | tr '[:upper:]' '[:lower:]')
            export ACR_LOGIN_SERVER="${ACR_LOGIN_SERVER_NORM}"
            export ACR_USERNAME="${{ secrets.ACR_USERNAME }}"
            export ACR_PASSWORD="${{ secrets.ACR_PASSWORD }}"
            export IMAGE_TAG="${{ env.IMAGE_TAG }}"

            export SERVICES_TO_DEPLOY="${{ needs.build_and_push.outputs.services_to_deploy }}"

            export ENV_FILE="${{ secrets.VM_ENV_FILE }}"
            # Pass services explicitly so compose only pulls/starts what we built.
            # shellcheck disable=SC2086
            sh ./scripts/vm-update.sh $SERVICES_TO_DEPLOY

  deploy_only:
    runs-on: ubuntu-latest
    if: ${{ github.event_name == 'workflow_dispatch' && inputs.mode == 'deploy_only' }}
    environment: ${{ (github.event_name == 'workflow_dispatch' && inputs.target == 'prod' && 'production') || (startsWith(github.ref, 'refs/tags/') && 'production') || 'staging' }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Validate VM deploy secrets present
        shell: bash
        run: |
          set -euo pipefail
          missing=0
          if [[ -z "${{ secrets.VM_HOST }}" ]]; then
            echo "::error::Missing secret VM_HOST (set it in repo secrets or in the selected environment secrets)"
            missing=1
          fi
          if [[ -z "${{ secrets.VM_USER }}" ]]; then
            echo "::error::Missing secret VM_USER"
            missing=1
          fi
          if [[ -z "${{ secrets.VM_SSH_KEY }}" ]]; then
            echo "::error::Missing secret VM_SSH_KEY"
            missing=1
          fi
          if [[ -z "${{ secrets.VM_DEPLOY_DIR }}" ]]; then
            echo "::error::Missing secret VM_DEPLOY_DIR"
            missing=1
          fi
          if [[ -z "${{ secrets.VM_ENV_FILE }}" ]]; then
            echo "::error::Missing secret VM_ENV_FILE"
            missing=1
          fi
          exit $missing

      - name: Ensure VM deploy dirs exist
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.VM_HOST }}
          username: ${{ secrets.VM_USER }}
          key: ${{ secrets.VM_SSH_KEY }}
          script: |
            set -e
            mkdir -p "${{ secrets.VM_DEPLOY_DIR }}" "${{ secrets.VM_DEPLOY_DIR }}/scripts" "${{ secrets.VM_DEPLOY_DIR }}/postgres/init"

            # Ensure deploy tree is writable by the SSH user (scp-action untars into target).
            if command -v sudo >/dev/null 2>&1; then
              sudo chown -R "${{ secrets.VM_USER }}":"${{ secrets.VM_USER }}" "${{ secrets.VM_DEPLOY_DIR }}" || true
              sudo chmod -R u+rwX,go+rX "${{ secrets.VM_DEPLOY_DIR }}" || true
            else
              chmod -R u+rwX,go+rX "${{ secrets.VM_DEPLOY_DIR }}" || true
            fi

      - name: Upload deploy assets to VM
        uses: appleboy/scp-action@v0.1.7
        with:
          host: ${{ secrets.VM_HOST }}
          username: ${{ secrets.VM_USER }}
          key: ${{ secrets.VM_SSH_KEY }}
          source: 'deploy/Caddyfile,deploy/docker-compose.prod.yml,deploy/docker-compose.prod.images.yml,deploy/docker-compose.pgadmin.yml'
          target: '${{ secrets.VM_DEPLOY_DIR }}'

      - name: Deploy config on VM (SSH, no build)
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.VM_HOST }}
          username: ${{ secrets.VM_USER }}
          key: ${{ secrets.VM_SSH_KEY }}
          script: |
            set -e
            cd "${{ secrets.VM_DEPLOY_DIR }}"

            ENV_FILE="${{ secrets.VM_ENV_FILE }}"

            echo "Caddyfile keycloak matcher preview:" 
            if [ -f ./Caddyfile ]; then
              grep -n -E "@keycloak|handle @keycloak|X-Forwarded-Proto|reverse_proxy keycloak" ./Caddyfile || true
            else
              echo "ERROR: ./Caddyfile not found in VM_DEPLOY_DIR" >&2
            fi

            # Apply config changes without pulling/building images.
            # NOTE: `caddy` depends_on `ocelot-gateway` in docker-compose.prod.yml.
            # In deploy-only mode we *must not* start dependencies, otherwise Compose may try
            # to recreate unrelated services and fail if their images are not present on the VM.
            docker compose --env-file "$ENV_FILE" \
              -f docker-compose.prod.yml \
              -f docker-compose.prod.images.yml \
              up -d --no-build --force-recreate --no-deps caddy keycloak

  bootstrap_minimal:
    runs-on: ubuntu-latest
    if: ${{ github.event_name == 'workflow_dispatch' && inputs.mode == 'bootstrap_minimal' }}
    environment: ${{ (github.event_name == 'workflow_dispatch' && inputs.target == 'prod' && 'production') || (startsWith(github.ref, 'refs/tags/') && 'production') || 'staging' }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Validate VM deploy secrets present
        shell: bash
        run: |
          set -euo pipefail
          missing=0
          if [[ -z "${{ secrets.VM_HOST }}" ]]; then
            echo "::error::Missing secret VM_HOST (set it in repo secrets or in the selected environment secrets)"
            missing=1
          fi
          if [[ -z "${{ secrets.VM_USER }}" ]]; then
            echo "::error::Missing secret VM_USER"
            missing=1
          fi
          if [[ -z "${{ secrets.VM_SSH_KEY }}" ]]; then
            echo "::error::Missing secret VM_SSH_KEY"
            missing=1
          fi
          if [[ -z "${{ secrets.VM_DEPLOY_DIR }}" ]]; then
            echo "::error::Missing secret VM_DEPLOY_DIR"
            missing=1
          fi
          if [[ -z "${{ secrets.VM_ENV_FILE }}" ]]; then
            echo "::error::Missing secret VM_ENV_FILE"
            missing=1
          fi
          exit $missing

      - name: Ensure VM deploy dirs exist
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.VM_HOST }}
          username: ${{ secrets.VM_USER }}
          key: ${{ secrets.VM_SSH_KEY }}
          script: |
            set -e
            mkdir -p \
              "${{ secrets.VM_DEPLOY_DIR }}" \
              "${{ secrets.VM_DEPLOY_DIR }}/scripts" \
              "${{ secrets.VM_DEPLOY_DIR }}/keycloak/import" \
              "${{ secrets.VM_DEPLOY_DIR }}/keycloak/keycloak-themes" \
              "${{ secrets.VM_DEPLOY_DIR }}/pgadmin"

            # Ensure deploy tree is writable by the SSH user (scp-action untars into target).
            if command -v sudo >/dev/null 2>&1; then
              sudo chown -R "${{ secrets.VM_USER }}":"${{ secrets.VM_USER }}" "${{ secrets.VM_DEPLOY_DIR }}" || true
              sudo chmod -R u+rwX,go+rX "${{ secrets.VM_DEPLOY_DIR }}" || true
            else
              chmod -R u+rwX,go+rX "${{ secrets.VM_DEPLOY_DIR }}" || true
            fi

      - name: Upload deploy assets to VM
        uses: appleboy/scp-action@v0.1.7
        with:
          host: ${{ secrets.VM_HOST }}
          username: ${{ secrets.VM_USER }}
          key: ${{ secrets.VM_SSH_KEY }}
          source: 'deploy/Caddyfile,deploy/docker-compose.prod.yml,deploy/docker-compose.prod.images.yml,deploy/docker-compose.pgadmin.yml'
          target: '${{ secrets.VM_DEPLOY_DIR }}'
          strip_components: 1
          overwrite: true

      - name: Upload Postgres init scripts to VM
        uses: appleboy/scp-action@v0.1.7
        with:
          host: ${{ secrets.VM_HOST }}
          username: ${{ secrets.VM_USER }}
          key: ${{ secrets.VM_SSH_KEY }}
          source: 'deploy/postgres/init/*'
          target: '${{ secrets.VM_DEPLOY_DIR }}/postgres/init'
          strip_components: 3
          overwrite: true

      - name: Upload Keycloak realm import to VM
        if: ${{ hashFiles('deploy/keycloak/import/**') != '' }}
        uses: appleboy/scp-action@v0.1.7
        with:
          host: ${{ secrets.VM_HOST }}
          username: ${{ secrets.VM_USER }}
          key: ${{ secrets.VM_SSH_KEY }}
          source: 'deploy/keycloak/import/*'
          target: '${{ secrets.VM_DEPLOY_DIR }}/keycloak/import'
          strip_components: 3
          overwrite: true

      - name: Bootstrap minimal stack on VM (SSH)
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.VM_HOST }}
          username: ${{ secrets.VM_USER }}
          key: ${{ secrets.VM_SSH_KEY }}
          script: |
            set -e
            cd "${{ secrets.VM_DEPLOY_DIR }}"

            ENV_FILE="${{ secrets.VM_ENV_FILE }}"

            echo "Resetting stack (containers + volumes) ..."
            docker compose --env-file "$ENV_FILE" \
              -f docker-compose.prod.yml \
              -f docker-compose.prod.images.yml \
              -f docker-compose.pgadmin.yml \
              down --remove-orphans

            # Preserve Caddy volumes to avoid re-issuing TLS certificates on every bootstrap.
            # Repeated full wipes can easily hit Let's Encrypt rate limits.
            PROJECT="${COMPOSE_PROJECT_NAME:-$(basename "$(pwd)")}" 
            echo "Compose project: $PROJECT"
            echo "Removing compose volumes (except caddy_data/caddy_config) ..."
            docker volume ls -q --filter "label=com.docker.compose.project=$PROJECT" | while read -r vol; do
              vkey=$(docker volume inspect "$vol" --format '{{ index .Labels "com.docker.compose.volume" }}' 2>/dev/null || true)
              if [ "$vkey" = "caddy_data" ] || [ "$vkey" = "caddy_config" ]; then
                echo "Keeping volume: $vol ($vkey)"
                continue
              fi
              echo "Removing volume: $vol (${vkey:-unknown})"
              docker volume rm "$vol" >/dev/null 2>&1 || true
            done

            echo "Removing Docker images and build cache ..."
            # After `down`, no compose containers should reference images, so prune can
            # clean aggressively without hitting 'image is in use' errors.
            docker system prune -af
            docker builder prune -af || true

            echo "Starting minimal services (postgres, redis, rabbitmq, keycloak) ..."
            docker compose --env-file "$ENV_FILE" \
              -f docker-compose.prod.yml \
              -f docker-compose.prod.images.yml \
              up -d --no-build postgres redis rabbitmq keycloak

            echo "Configuring Keycloak realm/client URLs from env ..."
            # Load env vars (DOMAIN, PUBLIC_BASE_URL, KEYCLOAK_ADMIN, etc.)
            set -a
            . "$ENV_FILE"
            set +a

            echo "Starting pgAdmin (staging default; VM-local only) ..."
            if [ -n "${PGADMIN_DEFAULT_EMAIL:-}" ]; then
              echo "PGADMIN_DEFAULT_EMAIL set: yes"
            else
              echo "PGADMIN_DEFAULT_EMAIL set: no"
            fi

            if [ -n "${PGADMIN_DEFAULT_PASSWORD:-}" ]; then
              echo "PGADMIN_DEFAULT_PASSWORD set: yes"
            else
              echo "PGADMIN_DEFAULT_PASSWORD set: no"
            fi

            if [ -n "${PGADMIN_DEFAULT_EMAIL:-}" ] && [ -n "${PGADMIN_DEFAULT_PASSWORD:-}" ]; then
              if [ ! -f ./docker-compose.pgadmin.yml ]; then
                echo "ERROR: docker-compose.pgadmin.yml not found in $(pwd)" >&2
                ls -la
                exit 1
              fi

              # Pre-register Postgres server in pgAdmin.
              # Note: server password is provided via .pgpass (VM-local file), not committed to git.
              printf '%s\n' "{\"Servers\":{\"1\":{\"Name\":\"exam-postgres\",\"Group\":\"ExamApp\",\"Host\":\"postgres\",\"Port\":5432,\"MaintenanceDB\":\"postgres\",\"Username\":\"${POSTGRES_USER}\",\"SSLMode\":\"prefer\"}}}" > ./pgadmin/servers.json

              # libpq will read this for passwordless connections inside pgAdmin.
              # Format: hostname:port:database:username:password
              printf '%s\n' "postgres:5432:*:${POSTGRES_USER}:${POSTGRES_PASSWORD}" > ./pgadmin/pgpass
              chmod 600 ./pgadmin/pgpass || true

              docker compose --env-file "$ENV_FILE" \
                -f docker-compose.prod.yml \
                -f docker-compose.pgadmin.yml \
                up -d --no-build pgadmin

              # Verify pgAdmin is actually running.
              if ! docker ps --format '{{.Names}}' | grep -qx 'exam-pgadmin'; then
                echo "ERROR: pgAdmin container did not start" >&2
                docker compose --env-file "$ENV_FILE" \
                  -f docker-compose.prod.yml \
                  -f docker-compose.pgadmin.yml \
                  ps || true
                docker logs --tail 200 exam-pgadmin || true
                exit 1
              fi
            else
              missing=""
              if [ -z "${PGADMIN_DEFAULT_EMAIL:-}" ]; then
                missing="${missing} PGADMIN_DEFAULT_EMAIL"
              fi
              if [ -z "${PGADMIN_DEFAULT_PASSWORD:-}" ]; then
                missing="${missing} PGADMIN_DEFAULT_PASSWORD"
              fi
              echo "Skipping pgAdmin: missing:${missing} (from $ENV_FILE)" >&2
            fi

            if [ -z "${PUBLIC_BASE_URL:-}" ]; then
              echo "ERROR: PUBLIC_BASE_URL is empty in $ENV_FILE" >&2
              exit 1
            fi
            if [ -z "${KEYCLOAK_ADMIN:-}" ] || [ -z "${KEYCLOAK_ADMIN_PASSWORD:-}" ]; then
              echo "ERROR: KEYCLOAK_ADMIN / KEYCLOAK_ADMIN_PASSWORD is empty in $ENV_FILE" >&2
              exit 1
            fi

            # Wait for Keycloak to be ready enough for admin API calls.
            i=0
            until docker exec exam-keycloak /opt/keycloak/bin/kcadm.sh config credentials \
              --server http://localhost:8080 \
              --realm master \
              --user "$KEYCLOAK_ADMIN" \
              --password "$KEYCLOAK_ADMIN_PASSWORD" >/dev/null 2>&1
            do
              i=$((i+1))
              if [ "$i" -ge 60 ]; then
                echo "ERROR: Keycloak did not become ready in time" >&2
                docker logs --tail 200 exam-keycloak || true
                exit 1
              fi
              sleep 2
            done

            # Set realm Frontend URL
            docker exec exam-keycloak /opt/keycloak/bin/kcadm.sh update realms/exam-realm \
              -s "attributes.frontendUrl=$PUBLIC_BASE_URL" >/dev/null

            # Update exam-client redirect URIs + web origins
            CLIENT_INTERNAL_ID=$(docker exec exam-keycloak /opt/keycloak/bin/kcadm.sh get clients \
              -r exam-realm \
              -q clientId=exam-client \
              --fields id | tr -d '\r' | sed -n 's/.*"id"[[:space:]]*:[[:space:]]*"\([^"]*\)".*/\1/p' | head -n 1)

            if [ -z "$CLIENT_INTERNAL_ID" ]; then
              echo "ERROR: Could not find clientId=exam-client in realm exam-realm" >&2
              exit 1
            fi

            docker exec exam-keycloak /opt/keycloak/bin/kcadm.sh update "clients/$CLIENT_INTERNAL_ID" \
              -r exam-realm \
              -s "redirectUris=[\"$PUBLIC_BASE_URL/app/*\",\"$PUBLIC_BASE_URL/*\"]" \
              -s "webOrigins=[\"$PUBLIC_BASE_URL\"]" >/dev/null

            echo "Starting caddy (no dependencies) ..."
            # `caddy` depends_on `ocelot-gateway`, which can recursively start the whole stack.
            # For bootstrap we want ONLY the minimal set.
            docker compose --env-file "$ENV_FILE" \
              -f docker-compose.prod.yml \
              -f docker-compose.prod.images.yml \
              up -d --no-build --no-deps caddy

            echo "Status:" 
            docker compose --env-file "$ENV_FILE" \
              -f docker-compose.prod.yml \
              -f docker-compose.prod.images.yml \
              ps

  diagnose_tls:
    runs-on: ubuntu-latest
    if: ${{ github.event_name == 'workflow_dispatch' && inputs.mode == 'diagnose_tls' }}
    environment: ${{ (github.event_name == 'workflow_dispatch' && inputs.target == 'prod' && 'production') || (startsWith(github.ref, 'refs/tags/') && 'production') || 'staging' }}

    steps:
      - name: Validate VM deploy secrets present
        shell: bash
        run: |
          set -euo pipefail
          missing=0
          if [[ -z "${{ secrets.VM_HOST }}" ]]; then
            echo "::error::Missing secret VM_HOST (set it in repo secrets or in the selected environment secrets)"
            missing=1
          fi
          if [[ -z "${{ secrets.VM_USER }}" ]]; then
            echo "::error::Missing secret VM_USER"
            missing=1
          fi
          if [[ -z "${{ secrets.VM_SSH_KEY }}" ]]; then
            echo "::error::Missing secret VM_SSH_KEY"
            missing=1
          fi
          if [[ -z "${{ secrets.VM_DEPLOY_DIR }}" ]]; then
            echo "::error::Missing secret VM_DEPLOY_DIR"
            missing=1
          fi
          if [[ -z "${{ secrets.VM_ENV_FILE }}" ]]; then
            echo "::error::Missing secret VM_ENV_FILE"
            missing=1
          fi
          exit $missing

      - name: Diagnose TLS on VM (SSH)
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.VM_HOST }}
          username: ${{ secrets.VM_USER }}
          key: ${{ secrets.VM_SSH_KEY }}
          script: |
            set -e
            cd "${{ secrets.VM_DEPLOY_DIR }}"

            ENV_FILE="${{ secrets.VM_ENV_FILE }}"
            echo "VM: $(hostname)"
            echo "Deploy dir: $(pwd)"
            echo "Env file: $ENV_FILE"

            if [ -f "$ENV_FILE" ]; then
              set -a
              . "$ENV_FILE"
              set +a
            else
              echo "WARNING: ENV_FILE not found on VM: $ENV_FILE" >&2
            fi

            echo "DOMAIN=${DOMAIN:-<empty>}"
            echo "PUBLIC_BASE_URL=${PUBLIC_BASE_URL:-<empty>}"

            echo "--- Port listeners (80/443) ---"
            if command -v sudo >/dev/null 2>&1; then
              sudo ss -lntp | grep -E ':(80|443)\b' || true
            else
              ss -lntp | grep -E ':(80|443)\b' || true
            fi

            echo "--- Docker containers (relevant) ---"
            docker ps --format 'table {{.Names}}\t{{.Status}}\t{{.Ports}}' | sed -n '1p;/exam-caddy/p;/exam-keycloak/p;/ocelot-gateway/p' || true

            echo "--- exam-caddy logs (tail) ---"
            docker logs --tail 250 exam-caddy || true

            echo "--- exam-caddy inspect (ports/mounts) ---"
            docker inspect exam-caddy --format '{{json .HostConfig.PortBindings}}' || true
            docker inspect exam-caddy --format '{{json .Mounts}}' || true

            echo "--- Caddyfile on VM (first 120 lines) ---"
            if [ -f ./Caddyfile ]; then
              sed -n '1,120p' ./Caddyfile
            else
              echo "ERROR: ./Caddyfile not found in $(pwd)" >&2
            fi

            echo "--- Validate Caddy config inside container ---"
            docker exec exam-caddy caddy validate --config /etc/caddy/Caddyfile || true

            echo "--- TLS handshake to localhost:443 (SNI=DOMAIN) ---"
            # Use a disposable container to ensure openssl exists.
            # network=host allows connecting to the VM's 127.0.0.1:443 directly.
            dom="${DOMAIN:-staging.hedefokul.com}"
            docker run --rm --network host alpine:3.20 sh -lc "apk add --no-cache openssl curl >/dev/null && echo | openssl s_client -connect 127.0.0.1:443 -servername $dom -tls1_2 -brief 2>&1 | sed -n '1,80p'" || true

            echo "--- Curl to localhost:443 (Host header) ---"
            docker run --rm --network host curlimages/curl:8.6.0 \
              -vkI --resolve "$dom:443:127.0.0.1" "https://$dom/" 2>&1 | sed -n '1,120p' || true

            echo "--- Compose status (if compose files exist) ---"
            if [ -f ./docker-compose.prod.yml ]; then
              docker compose --env-file "$ENV_FILE" \
                -f docker-compose.prod.yml \
                -f docker-compose.prod.images.yml \
                ps || true
            else
              echo "WARNING: docker-compose.prod.yml not found in $(pwd)" >&2
            fi
